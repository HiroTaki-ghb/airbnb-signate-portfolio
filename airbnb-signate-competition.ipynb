{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ea48eb-4faa-4738-b068-ef9a8abd5b9f",
   "metadata": {},
   "source": [
    "# Airbnb price prediction modeling competition hosted by SIGNATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d14a6-8c1b-4e27-8390-0988ea19ac42",
   "metadata": {},
   "source": [
    "* This competition is already terminated and is no longer accessible on the site\n",
    "* The submission with the following code was ranked 8th / 931 participants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90877583-42a7-4dd6-8e86-9714bb9f6ec3",
   "metadata": {},
   "source": [
    "### 1. Preparation : word extraction for the scoring of natural language columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fdff4-951f-4f66-bf4b-cba616183af5",
   "metadata": {},
   "source": [
    "##### 1-1. Extract words from the description column and the name column, and assign the median value to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749743a-afd8-462b-9627-68fa4c60c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df=pd.read_csv(\"train.csv\", index_col=0)\n",
    "\n",
    "#import libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "#Create descname columns by concatenating the description column and the name column\n",
    "df[\"descname\"] = df[\"description\"] + \" \" + df[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c198f0c-8a77-4af9-9929-923128709a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Victorization with binary = True to  を指定して、単語の重複を無視したベクトル化\n",
    "vectorizer = CountVectorizer(stop_words='english', binary=True)\n",
    "X = vectorizer.fit_transform(df['descname'])\n",
    "\n",
    "# Obtain the unique word list and the number of each word used\n",
    "word_counts = X.toarray().sum(axis=0)\n",
    "word_list = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the word list and each word's count into a DataFrame\n",
    "word_frequency = pd.DataFrame({'word': word_list, 'count': word_counts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd39e6-09fc-47bd-ae89-95fcfd7ab3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the price values list\n",
    "y_values = df['y'].values\n",
    "\n",
    "# Calculate the median value of y by the rows with each word in it\n",
    "median_prices = []\n",
    "for i, word in enumerate(word_list):\n",
    "    rows_with_word = X[:, i].toarray().flatten()  # rows that have a certain word\n",
    "    if rows_with_word.sum() > 0:  \n",
    "        relevant_y_values = y_values[rows_with_word == 1]  \n",
    "        median_price = np.median(relevant_y_values) \n",
    "    else:\n",
    "        median_price = np.nan  \n",
    "    median_prices.append(median_price)\n",
    "\n",
    "# Add the median prices in the DataFrame\n",
    "word_frequency['median_price'] = median_prices\n",
    "\n",
    "# Filter the words by the count >= 10\n",
    "filtered_word_frequency = word_frequency[word_frequency['count'] >= 10]\n",
    "\n",
    "# Export the DataFrame to .xlsx\n",
    "filtered_word_frequency.sort_values(by='count', ascending=False).to_excel('Wordlist.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418dd91d-fd59-4bef-8e6b-796e3f7c026e",
   "metadata": {},
   "source": [
    "##### 1-2. Decompose amenity column strings into words\n",
    "* *After exporting the dataframe, I gave a point to each amenity item manually on a scale of 0 to 4 by how luxurious the facility I thought it would imply.*<br>\n",
    "* *However, if it is too cumbersome, it is also possible to use the median value by item, with the same method as the description/name columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367a833-7d23-4790-8ce7-016aa93e9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "dic = defaultdict(int)\n",
    "\n",
    "for _ in range(df[\"amenities\"].shape[0]):\n",
    "  keys = re.findall(r'\\{(.*)\\}', df.loc[_,\"amenities\"])[0].split(\",\")\n",
    "  for key in keys:\n",
    "    dic[key] += 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# convert the dict into a DataFrame\n",
    "amenities_df = pd.DataFrame(list(dic.items()), columns=['amenity', 'count'])\n",
    "\n",
    "# Export the DataFrame into .xlsx file\n",
    "amenities_df.to_excel('amenity_scores.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1422382-c7e8-440e-a6b6-b1c3f414f960",
   "metadata": {},
   "source": [
    "### 2. Import necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b215f-2e3b-4f6d-81ba-d47d4824ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw=pd.read_csv(\"train.csv\", index_col=0)\n",
    "amenity_scores_df = pd.read_excel(\"amenity_scores.xlsx\")\n",
    "word_scores_df = pd.read_excel(\"Wordlist.xlsx\", sheet_name=\"Sheet1\")\n",
    "\n",
    "#Convert the scoring data to dict\n",
    "amenity_scores_dict = pd.Series(amenity_scores_df.score.values, index=amenity_scores_df.word).to_dict()\n",
    "word_scores_dict = pd.Series(word_scores_df.score.values, index=word_scores_df.word).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76238d5-9a2f-4cbc-a5dd-3bba35ee64cd",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessinng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea3777-6fb9-44b9-9ff2-53231313afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre(df):\n",
    "\n",
    "  #bathrooms,bedrooms,beds列の欠損値処理\n",
    "  for column in ['bathrooms', 'bedrooms', 'beds']:\n",
    "      if df[column].isnull().sum() > 0:\n",
    "          medians = df.groupby('accommodates')[column].median()\n",
    "\n",
    "          df[column] = df.apply(\n",
    "              lambda row: medians[row['accommodates']] if pd.isnull(row[column]) else row[column], axis=1\n",
    "          )\n",
    "\n",
    "  #review_scores_rating,host_identity_verified列の欠損値処理\n",
    "  df['review_scores_rating'] = df['review_scores_rating'].fillna(df['review_scores_rating'].median())\n",
    "  df['host_identity_verified'] = df['host_identity_verified'].fillna('f')\n",
    "  df[\"host_has_profile_pic\"] = df[\"host_has_profile_pic\"].fillna('f')\n",
    "\n",
    "  #host_response_rate列を数値変換＋欠損値処理\n",
    "  df[\"host_response_rate\"] = df[\"host_response_rate\"].str.rstrip('%').astype('float')\n",
    "  df['host_response_rate'] = df['host_response_rate'].fillna(df['host_response_rate'].median())\n",
    "\n",
    "  #thumbnail列を「入力あり=1,Null=0」に変換\n",
    "  df['thumbnail_url'] = df['thumbnail_url'].notna().astype(int)\n",
    "\n",
    "  #日付列の欠損値処理\n",
    "  df['host_since'] = pd.to_datetime(df['host_since'])\n",
    "  df['host_since']=df['host_since'].fillna(df['host_since'].median())\n",
    "  df[\"first_review\"] = pd.to_datetime(df[\"first_review\"])\n",
    "  df['first_review']=df['first_review'].fillna(df['host_since'])\n",
    "  df[\"last_review\"] = pd.to_datetime(df[\"last_review\"])\n",
    "  df['last_review']=df['last_review'].fillna(df['host_since'])\n",
    "\n",
    "  #日付列をfloat型変換\n",
    "  df['host_days_since'] = (pd.Timestamp('2017-10-05') - df['host_since']).dt.days\n",
    "  df[\"first_review\"] = df[\"first_review\"].astype('int64') // 10**9\n",
    "  df[\"last_review\"] = df[\"last_review\"].astype('int64') // 10**9\n",
    "\n",
    "  #amenity列のスコアリング\n",
    "  df['cleaned_amenities'] = df['amenities'].str.replace('\"', '', regex=False)\n",
    "  def calculate_amenity_score(cleaned_amenities):\n",
    "      total_score = 0\n",
    "      keys = re.findall(r'\\{(.*)\\}', cleaned_amenities)\n",
    "      if keys:\n",
    "          for key in keys[0].split(\",\"):\n",
    "              key = key.strip()\n",
    "              total_score += amenity_scores_dict.get(key, 0)\n",
    "      return total_score\n",
    "  df['amenity_scores'] = df['cleaned_amenities'].apply(calculate_amenity_score)\n",
    "\n",
    "  #name列のスコアリング\n",
    "  def calculate_name_score(name):\n",
    "      total_score = 0\n",
    "      words = name.split()  # nameを単語に分割\n",
    "      for word in words:\n",
    "          word = word.strip().lower()  # 比較を小文字で行うため、単語を小文字に変換\n",
    "          if word in word_scores_dict:\n",
    "              total_score += word_scores_dict[word]  # 単語が辞書にある場合、スコアを加算\n",
    "      return total_score\n",
    "  df['name_scores'] = df['name'].apply(calculate_name_score)\n",
    "\n",
    "  #description列のスコアリング\n",
    "  def calculate_description_score(description):\n",
    "      total_score = 0\n",
    "      words = description.split()\n",
    "      for word in words:\n",
    "          word = word.strip().lower()\n",
    "          if word in word_scores_dict:\n",
    "              total_score += word_scores_dict[word]\n",
    "      return total_score\n",
    "  df['description_scores'] = df['description'].apply(calculate_description_score)\n",
    "\n",
    "  #description_wordcount列の作成\n",
    "  df[\"description_wordcount\"] = df[\"description\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "  #不要なカラムを除去\n",
    "  drop_list=[\"amenities\",\"city\",\"description\",\"neighbourhood\",\"name\",'cleaned_amenities',\"host_since\",\"zipcode\"]\n",
    "  df=df.drop(drop_list,axis=1)\n",
    "\n",
    "  #object型のカラムをcategory型に変換\n",
    "  object_cols = df.select_dtypes(include='object').columns\n",
    "  df[object_cols] = df[object_cols].astype('category')\n",
    "\n",
    "  return df\n",
    "\n",
    "df=data_pre(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc37a4-eace-46de-99b4-96e16e17c59a",
   "metadata": {},
   "source": [
    "### 4. Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7abe1-b3c9-4285-ac1f-e5c18bee8eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as lgb_tune\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# データの分割\n",
    "df_train, df_val = train_test_split(df, test_size=0.2)\n",
    "\n",
    "col = \"y\"\n",
    "train_y = df_train[col]\n",
    "train_x = df_train.drop(col, axis=1)\n",
    "\n",
    "val_y = df_val[col]\n",
    "val_x = df_val.drop(col, axis=1)\n",
    "\n",
    "# LightGBMデータセットの作成\n",
    "trains = lgb.Dataset(train_x, train_y)\n",
    "valids = lgb.Dataset(val_x, val_y)\n",
    "\n",
    "# モデルパラメータの設定\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "}\n",
    "\n",
    "model_tune = lgb_tune.train(\n",
    "    params, trains, valid_sets=[valids],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(100),  # 早期停止を設定\n",
    "        lgb.log_evaluation(100)   # 100ラウンドごとにログを表示\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_preds = model_tune.predict(val_x)\n",
    "val_rmse = mean_squared_error(val_y, val_preds, squared=False)\n",
    "print(f'Validation RMSE: {val_rmse}')\n",
    "\n",
    "df_test=pd.read_csv(\"/content/drive/MyDrive/初回成果物/民泊価格予測/test.csv\", index_col=0)\n",
    "df_test=data_pre(df_test)\n",
    "\n",
    "predict = model_tune.predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37cb67-46e0-40ed-b175-cc6c15f2fe89",
   "metadata": {},
   "source": [
    "### 5. Postprocessing and export of the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27d06c-99dc-44e3-b2b2-f66830d8dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 予測値が10ドル以下の場合は10ドルに置き換える\n",
    "predict = [max(10, pred) for pred in predict]\n",
    "\n",
    "df_test[\"y\"] = predict\n",
    "df_test[\"y\"].to_csv(\"submission.csv\", header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
