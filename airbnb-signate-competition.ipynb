{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ea48eb-4faa-4738-b068-ef9a8abd5b9f",
   "metadata": {},
   "source": [
    "# Airbnb price prediction modeling competition hosted by SIGNATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d14a6-8c1b-4e27-8390-0988ea19ac42",
   "metadata": {},
   "source": [
    "* The competition is already terminated and is no longer accessible on the site\n",
    "* The submission with the following code was ranked 8th / 931 participants with RMSE around 140.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90877583-42a7-4dd6-8e86-9714bb9f6ec3",
   "metadata": {},
   "source": [
    "### 1. Preparation : word extraction for the scoring of natural language columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fdff4-951f-4f66-bf4b-cba616183af5",
   "metadata": {},
   "source": [
    "##### 1-1. Extract words from the description column and the name column, and assign the median value to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749743a-afd8-462b-9627-68fa4c60c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc8a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data from a CSV file, using the first column as the index\n",
    "df=pd.read_csv(\"train.csv\", index_col=0)\n",
    "\n",
    "# Create a new column 'descname' by concatenating 'description' and 'name' columns with a space separator\n",
    "df[\"descname\"] = df[\"description\"] + \" \" + df[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c198f0c-8a77-4af9-9929-923128709a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer to convert text to a binary matrix representation\n",
    "# binary=True means only presence/absence of a word is counted, ignoring multiple occurrences\n",
    "# stop_words='english' removes common English stop words\n",
    "vectorizer = CountVectorizer(stop_words='english', binary=True)\n",
    "\n",
    "# Fit the vectorizer on the 'descname' column and transform the text data into a sparse matrix\n",
    "X = vectorizer.fit_transform(df['descname'])\n",
    "\n",
    "# Calculate the total count of each unique word across all documents\n",
    "word_counts = X.toarray().sum(axis=0)\n",
    "\n",
    "# Get the list of unique words (features) extracted by the vectorizer\n",
    "word_list = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame with two columns: 'word' and its corresponding 'count'\n",
    "word_frequency = pd.DataFrame({'word': word_list, 'count': word_counts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd39e6-09fc-47bd-ae89-95fcfd7ab3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the price values array\n",
    "y_values = df['y'].values\n",
    "\n",
    "# Compute median price for each word\n",
    "median_prices = []\n",
    "for i, word in enumerate(word_list):\n",
    "    rows_with_word = X[:, i].toarray().flatten()  # rows that have a certain word\n",
    "    if rows_with_word.sum() > 0:  \n",
    "        relevant_y_values = y_values[rows_with_word == 1]  \n",
    "        median_price = np.median(relevant_y_values) \n",
    "    else:\n",
    "        median_price = np.nan  \n",
    "    median_prices.append(median_price)\n",
    "\n",
    "# Add the median prices in the DataFrame\n",
    "word_frequency['median_price'] = median_prices\n",
    "\n",
    "# Filter the words by the count >= 10\n",
    "filtered_word_frequency = word_frequency[word_frequency['count'] >= 10]\n",
    "\n",
    "# Export the DataFrame to .xlsx\n",
    "filtered_word_frequency.sort_values(by='count', ascending=False).to_excel('Wordlist.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418dd91d-fd59-4bef-8e6b-796e3f7c026e",
   "metadata": {},
   "source": [
    "##### 1-2. Decompose amenity column strings into words\n",
    "* *After exporting the dataframe, I gave a point to each amenity item manually on a scale of 0 to 4 by how luxurious the facility I thought it would imply.*<br>\n",
    "* *However, if it is too cumbersome, it is also possible to use the median value by item, with the same method as the description/name columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367a833-7d23-4790-8ce7-016aa93e9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a default dictionary to count the occurrence of each amenity\n",
    "dic = defaultdict(int)\n",
    "\n",
    "# Loop through each row in the 'amenities' column\n",
    "for _ in range(df[\"amenities\"].shape[0]):\n",
    "\n",
    "  # Extract the string inside curly braces, e.g., '{Wifi, TV}' -> 'Wifi, TV'\n",
    "  keys = re.findall(r'\\{(.*)\\}', df.loc[_,\"amenities\"])[0].split(\",\")\n",
    "  \n",
    "  # Split the string by commas and count each amenity\n",
    "  for key in keys:\n",
    "    dic[key] += 1\n",
    "\n",
    "# Convert the dictionary into a DataFrame\n",
    "amenities_df = pd.DataFrame(list(dic.items()), columns=['amenity', 'count'])\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "amenities_df.to_excel('amenity_scores.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1422382-c7e8-440e-a6b6-b1c3f414f960",
   "metadata": {},
   "source": [
    "### 2. Import necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b215f-2e3b-4f6d-81ba-d47d4824ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw training data and set the first column as the index\n",
    "df=pd.read_csv(\"train.csv\", index_col=0)\n",
    "\n",
    "# Load the amenity score and wordlist data from an Excel file\n",
    "amenity_scores_df = pd.read_excel(\"amenity_scores.xlsx\")\n",
    "word_scores_df = pd.read_excel(\"Wordlist.xlsx\")\n",
    "\n",
    "# Convert the scoring data to dictionary\n",
    "# For amenity, Key=item and value=score, and for wordlist, key = word and value = score \n",
    "amenity_scores_dict = pd.Series(amenity_scores_df.score.values, index=amenity_scores_df.word).to_dict()\n",
    "word_scores_dict = pd.Series(word_scores_df.score.values, index=word_scores_df.word).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76238d5-9a2f-4cbc-a5dd-3bba35ee64cd",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessinng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa8dfd",
   "metadata": {},
   "source": [
    "##### 3-1. Preprocess the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea3777-6fb9-44b9-9ff2-53231313afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre(df):\n",
    "  \n",
    "  # ====== Missing values imputation ======\n",
    "\n",
    "  # Fill missing values in bathroom/bedroom/bed columns by median value grouped by df['accommodates']\n",
    "  for column in ['bathrooms', 'bedrooms', 'beds']:\n",
    "      if df[column].isnull().sum() > 0:\n",
    "          medians = df.groupby('accommodates')[column].median()\n",
    "\n",
    "          df[column] = df.apply(\n",
    "              lambda row: medians[row['accommodates']] if pd.isnull(row[column]) else row[column], axis=1\n",
    "          )\n",
    "  # Fill missing rating with overall median\n",
    "  df['review_scores_rating'] = df['review_scores_rating'].fillna(df['review_scores_rating'].median())\n",
    "\n",
    "  # Fill missing categorical host data with 'f' (false)\n",
    "  df['host_identity_verified'] = df['host_identity_verified'].fillna('f')\n",
    "  df[\"host_has_profile_pic\"] = df[\"host_has_profile_pic\"].fillna('f')\n",
    "\n",
    "  # Clean and fill missing 'host_response_rate'\n",
    "  df[\"host_response_rate\"] = df[\"host_response_rate\"].str.rstrip('%').astype('float')\n",
    "  df['host_response_rate'] = df['host_response_rate'].fillna(df['host_response_rate'].median())\n",
    "  \n",
    "  # Convert availability of thumbnail URL into binary (1 if exists, 0 otherwise)\n",
    "  df['thumbnail_url'] = df['thumbnail_url'].notna().astype(int)\n",
    "\n",
    "\n",
    "  # ====== Handle datetime columns ======\n",
    "\n",
    "  # Convert 'host_since' to datetime and fill missing with median\n",
    "  df['host_since'] = pd.to_datetime(df['host_since'])\n",
    "  df['host_since']=df['host_since'].fillna(df['host_since'].median())\n",
    "\n",
    "  # Convert 'first_review' and 'last_review' to datetime\n",
    "  df[\"first_review\"] = pd.to_datetime(df[\"first_review\"])\n",
    "  df['first_review']=df['first_review'].fillna(df['host_since'])\n",
    "\n",
    "  df[\"last_review\"] = pd.to_datetime(df[\"last_review\"])\n",
    "  df['last_review']=df['last_review'].fillna(df['host_since'])\n",
    "\n",
    "  # Convert datetime to numeric values (days or UNIX timestamp)\n",
    "  df['host_days_since'] = (pd.Timestamp('2017-10-05') - df['host_since']).dt.days\n",
    "  df[\"first_review\"] = df[\"first_review\"].astype('int64') // 10**9\n",
    "  df[\"last_review\"] = df[\"last_review\"].astype('int64') // 10**9\n",
    "\n",
    "\n",
    "  # ====== Feature engineering ======\n",
    "\n",
    "  # Clean the amenities string\n",
    "  df['cleaned_amenities'] = df['amenities'].str.replace('\"', '', regex=False)\n",
    "\n",
    "  # Calculate the amenity score from predefined dictionary\n",
    "  def calculate_amenity_score(cleaned_amenities):\n",
    "      total_score = 0\n",
    "      keys = re.findall(r'\\{(.*)\\}', cleaned_amenities)\n",
    "      if keys:\n",
    "          for key in keys[0].split(\",\"):\n",
    "              key = key.strip()\n",
    "              total_score += amenity_scores_dict.get(key, 0)\n",
    "      return total_score\n",
    "  \n",
    "  df['amenity_scores'] = df['cleaned_amenities'].apply(calculate_amenity_score)\n",
    "\n",
    "  # Calculate the name score from predefined dictionary\n",
    "  def calculate_name_score(name):\n",
    "      total_score = 0\n",
    "      words = name.split() \n",
    "      for word in words:\n",
    "          word = word.strip().lower()  \n",
    "          if word in word_scores_dict:\n",
    "              total_score += word_scores_dict[word] \n",
    "      return total_score\n",
    "  df['name_scores'] = df['name'].apply(calculate_name_score)\n",
    "\n",
    "  # Calculate the description score from predefined dictionary\n",
    "  def calculate_description_score(description):\n",
    "      total_score = 0\n",
    "      words = description.split()\n",
    "      for word in words:\n",
    "          word = word.strip().lower()\n",
    "          if word in word_scores_dict:\n",
    "              total_score += word_scores_dict[word]\n",
    "      return total_score\n",
    "  df['description_scores'] = df['description'].apply(calculate_description_score)\n",
    "\n",
    "  # Count number of words in the 'description' field\n",
    "  df[\"description_wordcount\"] = df[\"description\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "\n",
    " # ====== Drop unused columns ======\n",
    "\n",
    "  drop_list=[\"amenities\",\"city\",\"description\",\"neighbourhood\",\"name\",'cleaned_amenities',\"host_since\",\"zipcode\"]\n",
    "  df=df.drop(drop_list,axis=1)\n",
    "\n",
    "\n",
    "  # Convert object-type columns to category dtype\n",
    "  object_cols = df.select_dtypes(include='object').columns\n",
    "  df[object_cols] = df[object_cols].astype('category')\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2047de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function\n",
    "df=data_pre(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ec327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Target encoding the training data ======\n",
    "\n",
    "# Get the categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['category', 'object']).columns\n",
    "\n",
    "# Prepare the empty dictionary to store target encoding mappings for each categorical column\n",
    "target_encodings = {}\n",
    "\n",
    "# Apply target encoding on training data\n",
    "for col in categorical_columns:\n",
    "    # Calculate the mean of the target valuable for each category\n",
    "    mean_prices = df.groupby(col)[\"y\"].mean()\n",
    "\n",
    "    # Create and store the mapping dictionary\n",
    "    target_encoding = {category: mean for category, mean in mean_prices.items()}\n",
    "    target_encodings[col] = target_encoding  # 保存\n",
    "\n",
    "    # Map categories to their mean target value and convert to float\n",
    "    df[f\"{col}_encoded\"] = df[col].map(target_encoding).astype(float)\n",
    "\n",
    "# Drop original categorical columns\n",
    "df = df.drop(columns=categorical_columns)\n",
    "\n",
    "# Display the result\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6309df45",
   "metadata": {},
   "source": [
    "##### 3-2. Preprocess the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b477fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data again and apply the predefined function\n",
    "# This DataFrame will be used for the target encoding of the test data\n",
    "df2 = pd.read_csv(\"train.csv\", index_col=0)\n",
    "df2 = data_pre(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9361c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and apply the predefined function to the test data\n",
    "df_test=pd.read_csv(\"test.csv\", index_col=0)\n",
    "df_test=data_pre(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Target encoding the test data ======\n",
    "\n",
    "# Identify categorical columns in the test data\n",
    "test_categorical_columns = df_test.select_dtypes(include=['category', 'object']).columns\n",
    "\n",
    "# Replace unseen categories in test data with 'Other'\n",
    "for col in test_categorical_columns:\n",
    "    # Get category values from training data\n",
    "    train_categories = df2[col].unique()\n",
    "\n",
    "    # Replace categories in test data not seen in training data with 'Other'\n",
    "    df_test[col] = df_test[col].apply(\n",
    "        lambda x: x if x in train_categories else 'Other'\n",
    "    )\n",
    "\n",
    "# Empty dictionary to store target encoding mappings for each categorical column\n",
    "test_target_encodings = {}\n",
    "\n",
    "# Apply target encoding to test data using training data statistics\n",
    "for col in test_categorical_columns:\n",
    "    # Calculate the mean of the taret variable for each category in training data\n",
    "    mean_prices = df2.groupby(col)[\"y\"].mean()\n",
    "\n",
    "    # Create and store the mapping dictionary\n",
    "    target_encoding = {category: mean for category, mean in mean_prices.items()}\n",
    "    target_encoding[\"Other\"]=df2[\"y\"].mean() #Use global mean for unseen categories\n",
    "    test_target_encodings[col] = target_encoding  \n",
    "\n",
    "    # Map categories to their mean target value and convert to float\n",
    "    df_test[f\"{col}_encoded\"] = df_test[col].map(target_encoding).astype(float)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "drop_list_test = df_test.select_dtypes(include=['category',\"object\"]).columns\n",
    "df_test = df_test.drop(columns=drop_list_test)\n",
    "\n",
    "# Display the result\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc37a4-eace-46de-99b4-96e16e17c59a",
   "metadata": {},
   "source": [
    "### 4. Data Modeling using LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7abe1-b3c9-4285-ac1f-e5c18bee8eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as lgb_tune\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "df_train, df_val = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Define target and feature columns for training data\n",
    "col = \"y\"\n",
    "train_y = df_train[col]\n",
    "train_x = df_train.drop(col, axis=1)\n",
    "\n",
    "# Define target and feature columns for validation data\n",
    "val_y = df_val[col]\n",
    "val_x = df_val.drop(col, axis=1)\n",
    "\n",
    "# Create LightGBM dataset objects\n",
    "trains = lgb.Dataset(train_x, train_y)\n",
    "valids = lgb.Dataset(val_x, val_y)\n",
    "\n",
    "# Set basic parameters for LightGBM with Optuna tuning\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "}\n",
    "\n",
    "# Train the model with Optuna-tuned LightGBM\n",
    "model_tune = lgb_tune.train(\n",
    "    params, trains, valid_sets=[valids],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(100),  \n",
    "        lgb.log_evaluation(100)   \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Predict on the validation set and calculate RMSE\n",
    "val_preds = model_tune.predict(val_x)\n",
    "val_rmse = mean_squared_error(val_y, val_preds, squared=False)\n",
    "print(f'Validation RMSE: {val_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ba4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the test dataset\n",
    "df_test=pd.read_csv(\"test.csv\", index_col=0)\n",
    "df_test=data_pre(df_test)\n",
    "\n",
    "# Predict on the test dataset\n",
    "predict = model_tune.predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37cb67-46e0-40ed-b175-cc6c15f2fe89",
   "metadata": {},
   "source": [
    "### 5. Postprocessing and export of the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27d06c-99dc-44e3-b2b2-f66830d8dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace predicted prices lower than 10 with 10 to handle unrealistic prediction\n",
    "predict = [max(10, pred) for pred in predict]\n",
    "\n",
    "# Assign the adjusted predictions back to the test DataFrame\n",
    "df_test[\"y\"] = predict\n",
    "\n",
    "# Export the predictions as a CSV file without header\n",
    "df_test[\"y\"].to_csv(\"submission.csv\", header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
